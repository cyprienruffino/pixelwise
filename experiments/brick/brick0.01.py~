import hashlib

import tensorflow as tf

import metrics.fid
from config import Config

from applications import upscaling_dilation_gen, disc
from datasets import from_hdf5
from utils import noise, synthetic


class CustomConfig(Config):

    def __init__(self, name):
        super().__init__(name)

        # Run metadata
        self.name = name
        self.seed = int(hashlib.sha1(name.encode("utf-8")).hexdigest(), 16) % (
            10 ** 8)

        # Training settings
        self.batch_size = 8
        self.epochs = 50
        self.k = 1  # Number of D updates vs G updates
        self.lmbda = 1
        self.validation = True
        self.test = True

        # Optimizers
        self.disc_optimizer = tf.train.AdamOptimizer
        self.disc_optimizer_args = {
            "learning_rate": 0.0002,
            "beta1": 0.5
        }
        self.gen_optimizer = tf.train.AdamOptimizer
        self.gen_optimizer_args = {
            "learning_rate": 0.0002,
            "beta1": 0.5
        }

        # Data dimensions
        self.channels = 3
        self.nz = 3  # Number of channels in Z
        self.zx = 6  # Size of each spatial dimensions in Z
        self.npx = 192  # (zx * 2^ depth)
        self.dataset_size = 20000
        self.valid_size = 5000
        self.test_size = 5000

        # Network setup
        self.generator = upscaling_dilation_gen.create_network
        self.gen_args = {
            "channels": self.channels
        }

        self.discriminator = disc.create_network
        self.disc_args = {
            "channels": self.channels
        }

        # Noise
        self.noise_provider = noise.uniform
        self.noise_provider_args = {
            "zx": self.zx,
            "nz": self.nz
        }

        self.fid_model = "fid_models/dtd.hdf5"
        self.metrics = {}
